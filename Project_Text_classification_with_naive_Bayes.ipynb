{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to know whether a review has a positive or negative rating for a movie, we can count the number of likes and dislikes in the reviews to see which type appears more frequently.\n",
    "\n",
    "In this project, I will perform naive Bayesian classification of the text of movie reviews and the ability to predict whether a movie review is positive or negative based on the review text. Also I want to check how Naive Bayes predicts which newsgroup a post is submitted to based on the text of the post.\n",
    "\n",
    "The naive Bayes formula I will use is:\n",
    "$$log\\left(P\\left(class\\left|text\\right|\\right)\\right)=log\\left(P\\left(text\\left|class\\right|\\right)\\cdot P\\left(class\\right)\\right)\\:$$\n",
    "\n",
    "This formula can be converted to $$log\\left(P\\left(text\\left|class\\right|\\right)\\right)+log\\left(P\\left(class\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training texts of \"movie_reviews.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part I'm going to check that the Naive Bayes classifier predicts whether a movie review is positive or negative, and check the accuracy of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "from collections import Counter\n",
    "movies = pd.read_csv(\"movie_reviews.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file movie_reviews.zip is a zipped csv file containing texts of about 25,000 movie reviews. Each review is accompanied by a label, indicating if the review is positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords.txt') as f:\n",
    "    stops = f.read()\n",
    "stops = stops.split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These words that make up the grammatical structure are meaningless, but will generate a lot of noise. Removing these \"noise\" words will improve the classification accuracy. We call these words \"stop words\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(movies, test_size=0.8, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = train_df[train_df[\"sentiment\"] == 'positive']\n",
    "dfn = train_df[train_df[\"sentiment\"] == 'negative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get two DataFrames containing all rows containing \"positive\" and \"negative\" in column \"sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "st = \"\"\n",
    "for i in range(0,len(dfp)):\n",
    "    s1 = dfp.iloc[i]['review'].lower()              # Convert all words in positive reviews to lowercase.\n",
    "    s1 = re.sub(r\"[\\\".,?;:)(-]|<br />\",'',s1)       # Remove punctuation.\n",
    "    st = st + \" \" + s1\n",
    "ls = st.split()                                     # Separate words.\n",
    "c1 = Counter(ls)                                    # Count the number of occurrences of each word in positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls1 = []\n",
    "st1 = \"\"\n",
    "for i in range(0,len(dfn)):\n",
    "    s1 = dfn.iloc[i]['review'].lower()              \n",
    "    s1 = re.sub(r\"[\\\".,?;:)(-]|<br />\",'',s1)    # Convert all words in negative reviews to lowercase.\n",
    "    st1 = st1 + \" \" + s1\n",
    "    \n",
    "ls1 = st1.split()\n",
    "c2 = Counter(ls1)                                # Count the number of occurrences of each word in negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>film</th>\n",
       "      <td>3865.0</td>\n",
       "      <td>3647.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie</th>\n",
       "      <td>3694.0</td>\n",
       "      <td>4659.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>2512.0</td>\n",
       "      <td>2452.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>very</th>\n",
       "      <td>1687.0</td>\n",
       "      <td>1135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>out</th>\n",
       "      <td>1606.0</td>\n",
       "      <td>1719.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cuteit</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rushed8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>destructs</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gyaos</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>let''s</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58733 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           positive  negative\n",
       "film         3865.0    3647.0\n",
       "movie        3694.0    4659.0\n",
       "one          2512.0    2452.0\n",
       "very         1687.0    1135.0\n",
       "out          1606.0    1719.0\n",
       "...             ...       ...\n",
       "cuteit          0.0       1.0\n",
       "rushed8         0.0       1.0\n",
       "destructs       0.0       1.0\n",
       "gyaos           0.0       1.0\n",
       "let''s          0.0       1.0\n",
       "\n",
       "[58733 rows x 2 columns]"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'positive': c1, 'negative': c2}\n",
    "df = pd.DataFrame(d)\n",
    "df = df.fillna(0)\n",
    "df = df.sort_values(\"positive\", ascending=False)\n",
    "df = df.drop(stops, errors='ignore')            # Remove \"stop words\".\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the number of occurrences of each word in \"positive reviews\" and \"negative reviews\" into a DataFrame, and remove \"stop words\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['positive-probility'] = df['positive']/df.sum()['positive']\n",
    "df['negative-probility'] = df['negative']/df.sum()['negative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the probability of each word appearing in different categories, which is $$ P\\left(class\\left|text\\right|\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive-probility</th>\n",
       "      <th>negative-probility</th>\n",
       "      <th>log(P(pos|text))</th>\n",
       "      <th>log(P(neg|text))</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>film</th>\n",
       "      <td>3865.0</td>\n",
       "      <td>3647.0</td>\n",
       "      <td>0.012388</td>\n",
       "      <td>0.012137</td>\n",
       "      <td>-2.470167</td>\n",
       "      <td>-2.495380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie</th>\n",
       "      <td>3694.0</td>\n",
       "      <td>4659.0</td>\n",
       "      <td>0.011839</td>\n",
       "      <td>0.015505</td>\n",
       "      <td>-2.489819</td>\n",
       "      <td>-2.389023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>2512.0</td>\n",
       "      <td>2452.0</td>\n",
       "      <td>0.008051</td>\n",
       "      <td>0.008160</td>\n",
       "      <td>-2.657296</td>\n",
       "      <td>-2.667796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>very</th>\n",
       "      <td>1687.0</td>\n",
       "      <td>1135.0</td>\n",
       "      <td>0.005407</td>\n",
       "      <td>0.003777</td>\n",
       "      <td>-2.830201</td>\n",
       "      <td>-3.002320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>out</th>\n",
       "      <td>1606.0</td>\n",
       "      <td>1719.0</td>\n",
       "      <td>0.005147</td>\n",
       "      <td>0.005721</td>\n",
       "      <td>-2.851571</td>\n",
       "      <td>-2.822040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prosecution</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-5.756286</td>\n",
       "      <td>-5.580195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>greenaway</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-5.756286</td>\n",
       "      <td>-5.580195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canvas</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-5.756286</td>\n",
       "      <td>-5.756286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smuggle</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-5.756286</td>\n",
       "      <td>-5.580195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eaten</th>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>-5.756286</td>\n",
       "      <td>-5.212218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10688 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             positive  negative  positive-probility  negative-probility  \\\n",
       "film           3865.0    3647.0            0.012388            0.012137   \n",
       "movie          3694.0    4659.0            0.011839            0.015505   \n",
       "one            2512.0    2452.0            0.008051            0.008160   \n",
       "very           1687.0    1135.0            0.005407            0.003777   \n",
       "out            1606.0    1719.0            0.005147            0.005721   \n",
       "...               ...       ...                 ...                 ...   \n",
       "prosecution       2.0       3.0            0.000006            0.000010   \n",
       "greenaway         2.0       3.0            0.000006            0.000010   \n",
       "canvas            2.0       2.0            0.000006            0.000007   \n",
       "smuggle           2.0       3.0            0.000006            0.000010   \n",
       "eaten             2.0       7.0            0.000006            0.000023   \n",
       "\n",
       "             log(P(pos|text))  log(P(neg|text))  \n",
       "film                -2.470167         -2.495380  \n",
       "movie               -2.489819         -2.389023  \n",
       "one                 -2.657296         -2.667796  \n",
       "very                -2.830201         -3.002320  \n",
       "out                 -2.851571         -2.822040  \n",
       "...                       ...               ...  \n",
       "prosecution         -5.756286         -5.580195  \n",
       "greenaway           -5.756286         -5.580195  \n",
       "canvas              -5.756286         -5.756286  \n",
       "smuggle             -5.756286         -5.580195  \n",
       "eaten               -5.756286         -5.212218  \n",
       "\n",
       "[10688 rows x 6 columns]"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['log(P(pos|text))'] = np.log10(df['positive-probility']) + np.log10((df.sum()['positive']/S))\n",
    "df['log(P(neg|text))'] = np.log10(df['negative-probility']) + np.log10((df.sum()['negative']/S))\n",
    "df1 = df[df[\"positive\"] > 1]\n",
    "df1 = df1[df1[\"negative\"] > 1]\n",
    "# Remove words that appear less than 1 time, which can make the prediction result more accurate.\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use products to compare probabilities, Python can't handle such small decimals, and they all end up being zero. So we're going to do it with logarithms, adding up the logarithms of each probability.\n",
    "\n",
    "$$log\\left(P\\left(text\\left|class\\right|\\right)\\cdot P\\left(class\\right)\\right)\\:$$ can be converted to:\n",
    "\n",
    "$$\\:log\\left(P\\left(text\\left|class\\right|\\right)\\right)+log\\left(P\\left(class\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test texts of \"movie_reviews.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(txt):\n",
    "    a = txt.lower()                                     # Convert all words in positive reviews to lowercase.\n",
    "    a = re.sub(r\"[\\\".,?;:)(-]|<br />\",'',a)             # Remove punctuation.\n",
    "    a = a.split()                                       # Separate words.\n",
    "    c4 = Counter(a)                                     # Count the number of occurrences of each word in the test review.\n",
    "    df3 = pd.DataFrame({'text':c4})\n",
    "    inner_merge = pd.merge(df1, df3,                    # Overlap common words.\n",
    "                       left_on=df1.index,\n",
    "                       right_on=df3.index,\n",
    "                       how=\"inner\")\n",
    "    return inner_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I wrote a function classify(txt) to find words that appear in both \"df1\" and \"test text\". I convert \"test review\" to lowercase and remove punctuation, get each word in \"test review\" and the number of times they appear through \"Counter\", and convert it to a DataFrame. Finally, use \"pd.merge\" to get the words they both have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_list = []\n",
    "def predict(x):\n",
    "    if x['log(P(pos|text))'] > x['log(P(neg|text))']:\n",
    "        predict_list.append(\"positive\")\n",
    "    else:\n",
    "        predict_list.append(\"negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By function classify I can get the probability table of words.\n",
    "\n",
    "Then, this function predict can collect the obtained prediction results to determine the classification with greater probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(test_df)):           # Make classification predictions for all reviews in test_df.\n",
    "    txt = test_df.iloc[i]['review']\n",
    "    result = classify(txt).sum()\n",
    "    predict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8405"
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predict_list == test_df['sentiment']).sum()/len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can say that using the Naive Bayes classifier predicts whether a movie review is positive or negative with 84.05% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training texts of \"newsgroups.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part I'm going to use how Naive Bayes predicts which newsgroup a post is submitted to based on the text of the post and check the accuracy of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile(\"newsgroups.zip\") as zipped:\n",
    "    txt0 = zipped.read('newsgroups.txt').decode(encoding='utf8', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_topics=[]\n",
    "a = re.findall(r\"Newsgroup: .*\\S\", txt0)       # Find all categories by \"Newsgroup\".\n",
    "for item in a:\n",
    "    list_topics.append(item.split(': ')[1])    # list of the different topics.\n",
    "\n",
    "\n",
    "\n",
    "l2 = []\n",
    "b = re.sub(r\"From: .*\\n\",'', txt0)             # Remove the \"From\" part of the texts.\n",
    "b = re.sub(r\"Subject: .*\\n\\n\\n\",'', b)         # Remove the \"Subject\" part of the texts.\n",
    "b = re.split(r\"Newsgroup: .*\\n\", b)            # Segment reviews according to \"Newsgroup\".\n",
    "\n",
    "list_review =[re.sub(r\"[#*&|\\\".,?$;:!<>+%=/)(]|\\n|-|\\t|\\d+\", '', i) for i in b]      # Remove punctuation.\n",
    "list_review.pop(0)                             # Remove the first empty element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_topics = {\"topics\":list_topics}\n",
    "d_review = {\"review\":list_review}\n",
    "df10 = pd.DataFrame(d_review)\n",
    "df11 = pd.DataFrame(d_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get two DataFrames containing all rows containing \"texts\" and \"topics\" in \"test_df\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_df = pd.concat([df11, df10], axis=1)   # Concatenate two DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "Newstrain_df, Newstest_df = train_test_split(newsgroups_df, test_size=0.8, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = Newstrain_df.groupby(\"topics\")\n",
    "# Group rows the DataFrame into parts, depending on the value of the “topics” column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the code below, I am trying to get the number of occurrences of words in each topics.First, I get the texts of each topic through \"grouped.get_group\", then convert all words in texts to lowercase, and finally use \"Counter\" to count the number of occurrences of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_rec_autos = []\n",
    "st_rec_autos = \"\"\n",
    "for i in range(0,len(grouped.get_group(\"rec.autos\"))):\n",
    "    s1 = grouped.get_group(\"rec.autos\").iloc[i]['review'].lower()  # Convert all words in reviews to lowercase.\n",
    "    st_rec_autos = st_rec_autos + \" \" + s1\n",
    "    \n",
    "ls_rec_autos = st_rec_autos.split()\n",
    "count_rec_autos = Counter(ls_rec_autos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_sci_med = []\n",
    "st_sci_med = \"\"\n",
    "for i in range(0,len(grouped.get_group(\"sci.med\"))):\n",
    "    s1 = grouped.get_group(\"sci.med\").iloc[i]['review'].lower()  # Convert all words in reviews to lowercase.\n",
    "    st_sci_med = st_sci_med + \" \" + s1\n",
    "    \n",
    "ls_sci_med = st_sci_med.split()\n",
    "count_sci_med = Counter(ls_sci_med)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_alt_atheism = []\n",
    "st_alt_atheism = \"\"\n",
    "for i in range(0,len(grouped.get_group(\"alt.atheism\"))):\n",
    "    s1 = grouped.get_group(\"alt.atheism\").iloc[i]['review'].lower()  # Convert all words in reviews to lowercase.\n",
    "    st_alt_atheism = st_alt_atheism + \" \" + s1\n",
    "    \n",
    "ls_alt_atheism = st_alt_atheism.split()\n",
    "count_alt_atheism = Counter(ls_alt_atheism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_rec_sport_baseball = []\n",
    "st_rec_sport_baseball = \"\"\n",
    "for i in range(0,len(grouped.get_group(\"rec.sport.baseball\"))):\n",
    "    s1 = grouped.get_group(\"rec.sport.baseball\").iloc[i]['review'].lower()  # Convert all words in reviews to lowercase.\n",
    "    st_rec_sport_baseball = st_rec_sport_baseball + \" \" + s1\n",
    "    \n",
    "ls_rec_sport_baseball = st_rec_sport_baseball.split()\n",
    "count_rec_sport_baseball = Counter(ls_rec_sport_baseball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_talk_religion_misc = []\n",
    "st_talk_religion_misc = \"\"\n",
    "for i in range(0,len(grouped.get_group(\"talk.religion.misc\"))):\n",
    "    s1 = grouped.get_group(\"talk.religion.misc\").iloc[i]['review'].lower()  # Convert all words in reviews to lowercase.\n",
    "    st_talk_religion_misc = st_talk_religion_misc + \" \" + s1\n",
    "    \n",
    "ls_talk_religion_misc = st_talk_religion_misc.split()\n",
    "count_talk_religion_misc = Counter(ls_talk_religion_misc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_sci_rec_sport_hockey = []\n",
    "st_sci_rec_sport_hockey = \"\"\n",
    "for i in range(0,len(grouped.get_group(\"rec.sport.hockey\"))):\n",
    "    s1 = grouped.get_group(\"rec.sport.hockey\").iloc[i]['review'].lower()  # Convert all words in reviews to lowercase.\n",
    "    st_sci_rec_sport_hockey = st_sci_rec_sport_hockey + \" \" + s1\n",
    "    \n",
    "ls_sci_rec_sport_hockey = st_sci_rec_sport_hockey.split()\n",
    "count_sci_rec_sport_hockey = Counter(ls_sci_rec_sport_hockey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_rec_sci_electronics = []\n",
    "st_rec_sci_electronics = \"\"\n",
    "for i in range(0,len(grouped.get_group(\"sci.electronics\"))):\n",
    "    s1 = grouped.get_group(\"sci.electronics\").iloc[i]['review'].lower()  # Convert all words in reviews to lowercase.\n",
    "    st_rec_sci_electronics = st_rec_sci_electronics + \" \" + s1\n",
    "    \n",
    "ls_rec_sci_electronics = st_rec_sci_electronics.split()\n",
    "count_rec_sci_electronics = Counter(ls_rec_sci_electronics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_rec_motorcycles = []\n",
    "st_rec_motorcycles = \"\"\n",
    "for i in range(0,len(grouped.get_group(\"rec.motorcycles\"))):\n",
    "    s1 = grouped.get_group(\"rec.motorcycles\").iloc[i]['review'].lower()  # Convert all words in reviews to lowercase.\n",
    "    st_rec_motorcycles = st_rec_motorcycles + \" \" + s1\n",
    "    \n",
    "ls_rec_motorcycles = st_rec_motorcycles.split()\n",
    "count_rec_motorcycles = Counter(ls_rec_motorcycles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I convert the resulting lists into a DataFrame and remove some \"noise words\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "Newsgroup_d = {'rec.autos': count_rec_autos, 'sci.med': count_sci_med, 'alt.atheism': count_alt_atheism,'rec.sport.baseball': count_rec_sport_baseball,'talk.religion.misc': count_talk_religion_misc,'rec.sport.hockey': count_sci_rec_sport_hockey,'sci.electronics': count_rec_sci_electronics,'rec.motorcycles': count_rec_motorcycles}\n",
    "Newsdf = pd.DataFrame(Newsgroup_d)\n",
    "Newsdf = Newsdf.fillna(0)\n",
    "\n",
    "Newsdf = Newsdf.drop(stops, errors='ignore')\n",
    "Newsdf = Newsdf[Newsdf[\"rec.autos\"] > 1]\n",
    "Newsdf = Newsdf[Newsdf[\"sci.med\"] > 1]\n",
    "Newsdf = Newsdf[Newsdf[\"alt.atheism\"] > 1]\n",
    "Newsdf = Newsdf[Newsdf[\"rec.sport.baseball\"] > 1]\n",
    "Newsdf = Newsdf[Newsdf[\"talk.religion.misc\"] > 1]\n",
    "Newsdf = Newsdf[Newsdf[\"rec.sport.hockey\"] > 1]\n",
    "Newsdf = Newsdf[Newsdf[\"sci.electronics\"] > 1]\n",
    "Newsdf = Newsdf[Newsdf[\"rec.motorcycles\"] > 1]\n",
    "# Remove words that appear less than 1 time, which can make the prediction result more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rec.autos</th>\n",
       "      <th>sci.med</th>\n",
       "      <th>alt.atheism</th>\n",
       "      <th>rec.sport.baseball</th>\n",
       "      <th>talk.religion.misc</th>\n",
       "      <th>rec.sport.hockey</th>\n",
       "      <th>sci.electronics</th>\n",
       "      <th>rec.motorcycles</th>\n",
       "      <th>rec.autos-probility</th>\n",
       "      <th>sci.med-probility</th>\n",
       "      <th>...</th>\n",
       "      <th>sci.electronics-probility</th>\n",
       "      <th>rec.motorcycles-probility</th>\n",
       "      <th>log(P(rec.autos|text))</th>\n",
       "      <th>log(P(sci.med|text))</th>\n",
       "      <th>log(P(alt.atheism|text))</th>\n",
       "      <th>log(P(rec.sport.baseball|text))</th>\n",
       "      <th>log(P(talk.religion.misc|text))</th>\n",
       "      <th>log(P(rec.sport.hockey|text))</th>\n",
       "      <th>log(P(sci.electronics|text))</th>\n",
       "      <th>log(P(rec.motorcycles|text))</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>o</th>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>0.001113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001105</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>-3.648493</td>\n",
       "      <td>-3.735643</td>\n",
       "      <td>-4.388856</td>\n",
       "      <td>-3.990916</td>\n",
       "      <td>-4.388856</td>\n",
       "      <td>-4.388856</td>\n",
       "      <td>-3.911735</td>\n",
       "      <td>-3.911735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <td>7.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>0.001979</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002578</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>-3.844788</td>\n",
       "      <td>-3.485766</td>\n",
       "      <td>-3.411132</td>\n",
       "      <td>-4.087826</td>\n",
       "      <td>-3.309675</td>\n",
       "      <td>-4.087826</td>\n",
       "      <td>-3.543758</td>\n",
       "      <td>-3.735643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>33.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.007324</td>\n",
       "      <td>0.003464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001657</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>-3.171372</td>\n",
       "      <td>-3.242728</td>\n",
       "      <td>-4.388856</td>\n",
       "      <td>-2.640668</td>\n",
       "      <td>-4.212765</td>\n",
       "      <td>-2.792259</td>\n",
       "      <td>-3.735643</td>\n",
       "      <td>-3.911735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>65.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.014425</td>\n",
       "      <td>0.006680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010127</td>\n",
       "      <td>0.008817</td>\n",
       "      <td>-2.876972</td>\n",
       "      <td>-2.957492</td>\n",
       "      <td>-2.934011</td>\n",
       "      <td>-2.755387</td>\n",
       "      <td>-2.982316</td>\n",
       "      <td>-2.832553</td>\n",
       "      <td>-2.949523</td>\n",
       "      <td>-3.027128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>information</th>\n",
       "      <td>10.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.006556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002946</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>-3.689886</td>\n",
       "      <td>-2.965610</td>\n",
       "      <td>-3.485766</td>\n",
       "      <td>-3.844788</td>\n",
       "      <td>-3.844788</td>\n",
       "      <td>-3.735643</td>\n",
       "      <td>-3.485766</td>\n",
       "      <td>-3.911735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ok</th>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>-4.212765</td>\n",
       "      <td>-3.911735</td>\n",
       "      <td>-3.689886</td>\n",
       "      <td>-3.911735</td>\n",
       "      <td>-4.087826</td>\n",
       "      <td>-3.990916</td>\n",
       "      <td>-4.388856</td>\n",
       "      <td>-3.786796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rd</th>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>-4.087826</td>\n",
       "      <td>-3.844788</td>\n",
       "      <td>-4.212765</td>\n",
       "      <td>-4.388856</td>\n",
       "      <td>-4.388856</td>\n",
       "      <td>-3.367666</td>\n",
       "      <td>-4.087826</td>\n",
       "      <td>-3.844788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>taken</th>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>-4.388856</td>\n",
       "      <td>-3.543758</td>\n",
       "      <td>-3.844788</td>\n",
       "      <td>-4.087826</td>\n",
       "      <td>-3.575942</td>\n",
       "      <td>-3.735643</td>\n",
       "      <td>-3.990916</td>\n",
       "      <td>-3.990916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michael</th>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002210</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>-4.388856</td>\n",
       "      <td>-3.543758</td>\n",
       "      <td>-4.212765</td>\n",
       "      <td>-3.735643</td>\n",
       "      <td>-3.575942</td>\n",
       "      <td>-3.990916</td>\n",
       "      <td>-3.610705</td>\n",
       "      <td>-3.844788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>move</th>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001105</td>\n",
       "      <td>0.001917</td>\n",
       "      <td>-4.388856</td>\n",
       "      <td>-3.844788</td>\n",
       "      <td>-3.990916</td>\n",
       "      <td>-3.990916</td>\n",
       "      <td>-4.388856</td>\n",
       "      <td>-3.388856</td>\n",
       "      <td>-3.911735</td>\n",
       "      <td>-3.689886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>370 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             rec.autos  sci.med  alt.atheism  rec.sport.baseball  \\\n",
       "o                 11.0      9.0          2.0                 5.0   \n",
       "set                7.0     16.0         19.0                 4.0   \n",
       "year              33.0     28.0          2.0               112.0   \n",
       "good              65.0     54.0         57.0                86.0   \n",
       "information       10.0     53.0         16.0                 7.0   \n",
       "...                ...      ...          ...                 ...   \n",
       "ok                 3.0      6.0         10.0                 6.0   \n",
       "rd                 4.0      7.0          3.0                 2.0   \n",
       "taken              2.0     14.0          7.0                 4.0   \n",
       "michael            2.0     14.0          3.0                 9.0   \n",
       "move               2.0      7.0          5.0                 5.0   \n",
       "\n",
       "             talk.religion.misc  rec.sport.hockey  sci.electronics  \\\n",
       "o                           2.0               2.0              6.0   \n",
       "set                        24.0               4.0             14.0   \n",
       "year                        3.0              79.0              9.0   \n",
       "good                       51.0              72.0             55.0   \n",
       "information                 7.0               9.0             16.0   \n",
       "...                         ...               ...              ...   \n",
       "ok                          4.0               5.0              2.0   \n",
       "rd                          2.0              21.0              4.0   \n",
       "taken                      13.0               9.0              5.0   \n",
       "michael                    13.0               5.0             12.0   \n",
       "move                        2.0              20.0              6.0   \n",
       "\n",
       "             rec.motorcycles  rec.autos-probility  sci.med-probility  ...  \\\n",
       "o                        6.0             0.002441           0.001113  ...   \n",
       "set                      9.0             0.001553           0.001979  ...   \n",
       "year                     6.0             0.007324           0.003464  ...   \n",
       "good                    46.0             0.014425           0.006680  ...   \n",
       "information              6.0             0.002219           0.006556  ...   \n",
       "...                      ...                  ...                ...  ...   \n",
       "ok                       8.0             0.000666           0.000742  ...   \n",
       "rd                       7.0             0.000888           0.000866  ...   \n",
       "taken                    5.0             0.000444           0.001732  ...   \n",
       "michael                  7.0             0.000444           0.001732  ...   \n",
       "move                    10.0             0.000444           0.000866  ...   \n",
       "\n",
       "             sci.electronics-probility  rec.motorcycles-probility  \\\n",
       "o                             0.001105                   0.001150   \n",
       "set                           0.002578                   0.001725   \n",
       "year                          0.001657                   0.001150   \n",
       "good                          0.010127                   0.008817   \n",
       "information                   0.002946                   0.001150   \n",
       "...                                ...                        ...   \n",
       "ok                            0.000368                   0.001533   \n",
       "rd                            0.000737                   0.001342   \n",
       "taken                         0.000921                   0.000958   \n",
       "michael                       0.002210                   0.001342   \n",
       "move                          0.001105                   0.001917   \n",
       "\n",
       "             log(P(rec.autos|text))  log(P(sci.med|text))  \\\n",
       "o                         -3.648493             -3.735643   \n",
       "set                       -3.844788             -3.485766   \n",
       "year                      -3.171372             -3.242728   \n",
       "good                      -2.876972             -2.957492   \n",
       "information               -3.689886             -2.965610   \n",
       "...                             ...                   ...   \n",
       "ok                        -4.212765             -3.911735   \n",
       "rd                        -4.087826             -3.844788   \n",
       "taken                     -4.388856             -3.543758   \n",
       "michael                   -4.388856             -3.543758   \n",
       "move                      -4.388856             -3.844788   \n",
       "\n",
       "             log(P(alt.atheism|text))  log(P(rec.sport.baseball|text))  \\\n",
       "o                           -4.388856                        -3.990916   \n",
       "set                         -3.411132                        -4.087826   \n",
       "year                        -4.388856                        -2.640668   \n",
       "good                        -2.934011                        -2.755387   \n",
       "information                 -3.485766                        -3.844788   \n",
       "...                               ...                              ...   \n",
       "ok                          -3.689886                        -3.911735   \n",
       "rd                          -4.212765                        -4.388856   \n",
       "taken                       -3.844788                        -4.087826   \n",
       "michael                     -4.212765                        -3.735643   \n",
       "move                        -3.990916                        -3.990916   \n",
       "\n",
       "             log(P(talk.religion.misc|text))  log(P(rec.sport.hockey|text))  \\\n",
       "o                                  -4.388856                      -4.388856   \n",
       "set                                -3.309675                      -4.087826   \n",
       "year                               -4.212765                      -2.792259   \n",
       "good                               -2.982316                      -2.832553   \n",
       "information                        -3.844788                      -3.735643   \n",
       "...                                      ...                            ...   \n",
       "ok                                 -4.087826                      -3.990916   \n",
       "rd                                 -4.388856                      -3.367666   \n",
       "taken                              -3.575942                      -3.735643   \n",
       "michael                            -3.575942                      -3.990916   \n",
       "move                               -4.388856                      -3.388856   \n",
       "\n",
       "             log(P(sci.electronics|text))  log(P(rec.motorcycles|text))  \n",
       "o                               -3.911735                     -3.911735  \n",
       "set                             -3.543758                     -3.735643  \n",
       "year                            -3.735643                     -3.911735  \n",
       "good                            -2.949523                     -3.027128  \n",
       "information                     -3.485766                     -3.911735  \n",
       "...                                   ...                           ...  \n",
       "ok                              -4.388856                     -3.786796  \n",
       "rd                              -4.087826                     -3.844788  \n",
       "taken                           -3.990916                     -3.990916  \n",
       "michael                         -3.610705                     -3.844788  \n",
       "move                            -3.911735                     -3.689886  \n",
       "\n",
       "[370 rows x 24 columns]"
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Newsdf['rec.autos-probility'] = Newsdf['rec.autos']/Newsdf.sum()['rec.autos']\n",
    "Newsdf['sci.med-probility'] = Newsdf['sci.med']/Newsdf.sum()['sci.med']\n",
    "Newsdf['alt.atheism-probility'] = Newsdf['alt.atheism']/Newsdf.sum()['alt.atheism']\n",
    "Newsdf['rec.sport.baseball-probility'] = Newsdf['rec.sport.baseball']/Newsdf.sum()['rec.sport.baseball']\n",
    "Newsdf['talk.religion.misc-probility'] = Newsdf['talk.religion.misc']/Newsdf.sum()['talk.religion.misc']\n",
    "Newsdf['rec.sport.hockey-probility'] = Newsdf['rec.sport.hockey']/Newsdf.sum()['rec.sport.hockey']\n",
    "Newsdf['sci.electronics-probility'] = Newsdf['sci.electronics']/Newsdf.sum()['sci.electronics']\n",
    "Newsdf['rec.motorcycles-probility'] = Newsdf['rec.motorcycles']/Newsdf.sum()['rec.motorcycles']\n",
    "# Calculate the probability of each word appearing in different categories.\n",
    "\n",
    "Su = Newsdf.sum()['rec.autos']+Newsdf.sum()['sci.med']+Newsdf.sum()['alt.atheism']+Newsdf.sum()['rec.sport.baseball']+Newsdf.sum()['talk.religion.misc']+Newsdf.sum()['rec.sport.hockey']+Newsdf.sum()['sci.electronics']+Newsdf.sum()['rec.motorcycles']\n",
    "# Occurrences of all categories\n",
    "\n",
    "Newsdf['log(P(rec.autos|text))'] = np.log10(Newsdf['rec.autos-probility']) + np.log10((Newsdf.sum()['rec.autos']/Su))\n",
    "Newsdf['log(P(sci.med|text))'] = np.log10(Newsdf['sci.med-probility']) + np.log10((Newsdf.sum()['sci.med']/Su))\n",
    "Newsdf['log(P(alt.atheism|text))'] = np.log10(Newsdf['alt.atheism-probility']) + np.log10((Newsdf.sum()['alt.atheism']/Su))\n",
    "Newsdf['log(P(rec.sport.baseball|text))'] = np.log10(Newsdf['rec.sport.baseball-probility']) + np.log10((Newsdf.sum()['rec.sport.baseball']/Su))\n",
    "Newsdf['log(P(talk.religion.misc|text))'] = np.log10(Newsdf['talk.religion.misc-probility']) + np.log10((Newsdf.sum()['talk.religion.misc']/Su))\n",
    "Newsdf['log(P(rec.sport.hockey|text))'] = np.log10(Newsdf['rec.sport.hockey-probility']) + np.log10((Newsdf.sum()['rec.sport.hockey']/Su))\n",
    "Newsdf['log(P(sci.electronics|text))'] = np.log10(Newsdf['sci.electronics-probility']) + np.log10((Newsdf.sum()['sci.electronics']/Su))\n",
    "Newsdf['log(P(rec.motorcycles|text))'] = np.log10(Newsdf['rec.motorcycles-probility']) + np.log10((Newsdf.sum()['rec.motorcycles']/Su))\n",
    "Newsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use products to compare probabilities, Python can't handle such small decimals, and they all end up being zero. So we're going to do it with logarithms, adding up the logarithms of each probability, which is $$\\:log\\left(P\\left(text\\left|class\\right|\\right)\\right)+log\\left(P\\left(class\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test texts of \"newsgroups.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, first I wrote a function classify_Newsgroup(txt) to find words that appear in both \"Newsdf\" and \"test text\". I convert \"test review\" to lowercase and remove punctuation, get each word in \"test review\" and the number of times they appear through \"Counter\", and convert it to a DataFrame. Finally, use \"pd.merge\" to get the words they both have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_Newsgroup(txt):\n",
    "    a = txt.lower()                         # Convert all words in positive reviews to lowercase.\n",
    "    a = re.sub(r\"[#*&|\\\".,?;:!<>+%=/)(]|\\n|-|\\t|\\d+\", '', a)  # Remove punctuation.\n",
    "    a = a.split()                           # Seperate words\n",
    "    c4 = Counter(a)                         # Count the number of occurrences of each word in the test review.\n",
    "    df3 = pd.DataFrame({'text':c4})\n",
    "    inner_merge = pd.merge(Newsdf, df3,     # Overlap common words.\n",
    "                       left_on=Newsdf.index,\n",
    "                       right_on=df3.index,\n",
    "                       how=\"inner\")\n",
    "    return inner_merge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By function classify_Newsgroup I can get the probability table of words.\n",
    "Then, this function predict can collect the obtained prediction results to determine the classification with greater probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "Newsgroup_predict_list = []\n",
    "def predict_Newsgroup(x):\n",
    "    if x['log(P(rec.autos|text))'] == max(x[17:25]):\n",
    "        Newsgroup_predict_list.append(\"rec.autos\")\n",
    "    elif x['log(P(sci.med|text))'] == max(x[17:25]):\n",
    "        Newsgroup_predict_list.append(\"sci.med\")\n",
    "    elif x['log(P(alt.atheism|text))'] == max(x[17:25]):\n",
    "        Newsgroup_predict_list.append(\"alt.atheism\")\n",
    "    elif x['log(P(rec.sport.baseball|text))'] == max(x[17:25]):\n",
    "        Newsgroup_predict_list.append(\"rec.sport.baseball\")\n",
    "    elif x['log(P(talk.religion.misc|text))'] == max(x[17:25]):\n",
    "        Newsgroup_predict_list.append(\"talk.religion.misc\")\n",
    "    elif x['log(P(rec.sport.hockey|text))'] == max(x[17:25]):\n",
    "        Newsgroup_predict_list.append(\"rec.sport.hockey\")\n",
    "    elif x['log(P(sci.electronics|text))'] == max(x[17:25]):\n",
    "        Newsgroup_predict_list.append(\"sci.electronics\")\n",
    "    elif x['log(P(rec.motorcycles|text))'] == max(x[17:25]):\n",
    "        Newsgroup_predict_list.append(\"rec.motorcycles\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is to collect the obtained prediction results and determine the topics with more probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make classification predictions for all reviews in test_df.\n",
    "for i in range(0,len(Newstest_df)):\n",
    "    tx = Newstest_df.iloc[i]['review']\n",
    "    result10 = classify_Newsgroup(tx).sum()\n",
    "    predict_Newsgroup(result10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2317465695409114"
      ]
     },
     "execution_count": 621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Newsgroup_predict_list == Newstest_df['topics']).sum()/len(Newstest_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can say that using the Naive Bayes classifier predicts the topics of reviews with 23.17% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most frequent in each class of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive-probility</th>\n",
       "      <th>negative-probility</th>\n",
       "      <th>log(P(pos|text))</th>\n",
       "      <th>log(P(neg|text))</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>film</th>\n",
       "      <td>3865.0</td>\n",
       "      <td>3647.0</td>\n",
       "      <td>0.012388</td>\n",
       "      <td>0.012137</td>\n",
       "      <td>-2.470167</td>\n",
       "      <td>-2.495380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie</th>\n",
       "      <td>3694.0</td>\n",
       "      <td>4659.0</td>\n",
       "      <td>0.011839</td>\n",
       "      <td>0.015505</td>\n",
       "      <td>-2.489819</td>\n",
       "      <td>-2.389023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>2512.0</td>\n",
       "      <td>2452.0</td>\n",
       "      <td>0.008051</td>\n",
       "      <td>0.008160</td>\n",
       "      <td>-2.657296</td>\n",
       "      <td>-2.667796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>very</th>\n",
       "      <td>1687.0</td>\n",
       "      <td>1135.0</td>\n",
       "      <td>0.005407</td>\n",
       "      <td>0.003777</td>\n",
       "      <td>-2.830201</td>\n",
       "      <td>-3.002320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>out</th>\n",
       "      <td>1606.0</td>\n",
       "      <td>1719.0</td>\n",
       "      <td>0.005147</td>\n",
       "      <td>0.005721</td>\n",
       "      <td>-2.851571</td>\n",
       "      <td>-2.822040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spared</th>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>-5.756286</td>\n",
       "      <td>-5.212218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>identification</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-5.756286</td>\n",
       "      <td>-5.756286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sparked</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-5.756286</td>\n",
       "      <td>-5.756286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weaving</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-5.756286</td>\n",
       "      <td>-5.580195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eaten</th>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>-5.756286</td>\n",
       "      <td>-5.212218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10688 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                positive  negative  positive-probility  negative-probility  \\\n",
       "film              3865.0    3647.0            0.012388            0.012137   \n",
       "movie             3694.0    4659.0            0.011839            0.015505   \n",
       "one               2512.0    2452.0            0.008051            0.008160   \n",
       "very              1687.0    1135.0            0.005407            0.003777   \n",
       "out               1606.0    1719.0            0.005147            0.005721   \n",
       "...                  ...       ...                 ...                 ...   \n",
       "spared               2.0       7.0            0.000006            0.000023   \n",
       "identification       2.0       2.0            0.000006            0.000007   \n",
       "sparked              2.0       2.0            0.000006            0.000007   \n",
       "weaving              2.0       3.0            0.000006            0.000010   \n",
       "eaten                2.0       7.0            0.000006            0.000023   \n",
       "\n",
       "                log(P(pos|text))  log(P(neg|text))  \n",
       "film                   -2.470167         -2.495380  \n",
       "movie                  -2.489819         -2.389023  \n",
       "one                    -2.657296         -2.667796  \n",
       "very                   -2.830201         -3.002320  \n",
       "out                    -2.851571         -2.822040  \n",
       "...                          ...               ...  \n",
       "spared                 -5.756286         -5.212218  \n",
       "identification         -5.756286         -5.756286  \n",
       "sparked                -5.756286         -5.756286  \n",
       "weaving                -5.756286         -5.580195  \n",
       "eaten                  -5.756286         -5.212218  \n",
       "\n",
       "[10688 rows x 6 columns]"
      ]
     },
     "execution_count": 719,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5 = df1.sort_values(\"positive\", ascending=False)\n",
    "# Use sort_values to sort the DataFrame according to \"positive\", from largest to smallest.\n",
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the DataFrame, we can know that the 5 most frequent words in positive reviews are: \"film\", \"movie\", \"one\", \"very\", \"out\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive-probility</th>\n",
       "      <th>negative-probility</th>\n",
       "      <th>log(P(pos|text))</th>\n",
       "      <th>log(P(neg|text))</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>movie</th>\n",
       "      <td>3694.0</td>\n",
       "      <td>4659.0</td>\n",
       "      <td>0.011839</td>\n",
       "      <td>0.015505</td>\n",
       "      <td>-2.489819</td>\n",
       "      <td>-2.389023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>film</th>\n",
       "      <td>3865.0</td>\n",
       "      <td>3647.0</td>\n",
       "      <td>0.012388</td>\n",
       "      <td>0.012137</td>\n",
       "      <td>-2.470167</td>\n",
       "      <td>-2.495380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>2512.0</td>\n",
       "      <td>2452.0</td>\n",
       "      <td>0.008051</td>\n",
       "      <td>0.008160</td>\n",
       "      <td>-2.657296</td>\n",
       "      <td>-2.667796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>out</th>\n",
       "      <td>1606.0</td>\n",
       "      <td>1719.0</td>\n",
       "      <td>0.005147</td>\n",
       "      <td>0.005721</td>\n",
       "      <td>-2.851571</td>\n",
       "      <td>-2.822040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it's</th>\n",
       "      <td>1591.0</td>\n",
       "      <td>1663.0</td>\n",
       "      <td>0.005099</td>\n",
       "      <td>0.005534</td>\n",
       "      <td>-2.855646</td>\n",
       "      <td>-2.836424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he/she</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-5.580195</td>\n",
       "      <td>-5.756286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prejudices</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-5.580195</td>\n",
       "      <td>-5.756286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accountant</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-5.580195</td>\n",
       "      <td>-5.756286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blasting</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-5.580195</td>\n",
       "      <td>-5.756286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>julian's</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-5.455256</td>\n",
       "      <td>-5.756286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10688 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            positive  negative  positive-probility  negative-probility  \\\n",
       "movie         3694.0    4659.0            0.011839            0.015505   \n",
       "film          3865.0    3647.0            0.012388            0.012137   \n",
       "one           2512.0    2452.0            0.008051            0.008160   \n",
       "out           1606.0    1719.0            0.005147            0.005721   \n",
       "it's          1591.0    1663.0            0.005099            0.005534   \n",
       "...              ...       ...                 ...                 ...   \n",
       "he/she           3.0       2.0            0.000010            0.000007   \n",
       "prejudices       3.0       2.0            0.000010            0.000007   \n",
       "accountant       3.0       2.0            0.000010            0.000007   \n",
       "blasting         3.0       2.0            0.000010            0.000007   \n",
       "julian's         4.0       2.0            0.000013            0.000007   \n",
       "\n",
       "            log(P(pos|text))  log(P(neg|text))  \n",
       "movie              -2.489819         -2.389023  \n",
       "film               -2.470167         -2.495380  \n",
       "one                -2.657296         -2.667796  \n",
       "out                -2.851571         -2.822040  \n",
       "it's               -2.855646         -2.836424  \n",
       "...                      ...               ...  \n",
       "he/she             -5.580195         -5.756286  \n",
       "prejudices         -5.580195         -5.756286  \n",
       "accountant         -5.580195         -5.756286  \n",
       "blasting           -5.580195         -5.756286  \n",
       "julian's           -5.455256         -5.756286  \n",
       "\n",
       "[10688 rows x 6 columns]"
      ]
     },
     "execution_count": 720,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6 = df1.sort_values(\"negative\", ascending=False)  \n",
    "# Use sort_values to sort the DataFrame according to \"negative\", from largest to smallest.\n",
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the DataFrame, we can know that the 5 most frequent words in negative reviews are: \"movie\", \"film\", \"one\", \"out\", \"it's\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x =list(range(5))\n",
    "total_width, n = 0.8, 2\n",
    "width = total_width / n\n",
    " \n",
    "plt.bar(x, df5.head(5)['positive'], width=width, label='positive',color = 'b')\n",
    "for i in range(len(x)):\n",
    "    x[i] = x[i] + width\n",
    "plt.bar(x, df5.head(5)['negative'], width=width, label='girl',tick_label = df5.head(5).index,color = 'g')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows the frequency of \"film\", \"movie\", \"one\", \"very\", \"out\" in \"positive\" and \"negative\". As can be seen from the plot, the five most frequently occurring words in positive reviews also appear a lot in negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze examples of texts which have been misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_df = test_df.loc[predict_list != test_df['sentiment']]\n",
    "# Collect false predicted rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I calculated the predicted probabilities in the false predicted rows through the function classify and added them to a new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tp = []\n",
    "Tn = []\n",
    "analytsis = false_df.reset_index()\n",
    "for i in range(0,len(false_df)):\n",
    "    txt1 = false_df.iloc[i]['review']\n",
    "    Tp.append(classify(txt1).sum()['log(P(pos|text))'])\n",
    "    Tn.append(classify(txt1).sum()['log(P(neg|text))'])\n",
    "\n",
    "analytsis['log(P(pos|text))'] = pd.DataFrame(Tp)\n",
    "analytsis['log(P(neg|text))'] = pd.DataFrame(Tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytsis['log_Probility_error'] = analytsis['log(P(pos|text))'] - analytsis['log(P(neg|text))']\n",
    "# Storing the error of log(probility) in analytsis['log_Probility_error']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>log(P(pos|text))</th>\n",
       "      <th>log(P(neg|text))</th>\n",
       "      <th>log_Probility_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21492</td>\n",
       "      <td>I saw this recent Woody Allen film because I'm...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-382.490069</td>\n",
       "      <td>-385.056348</td>\n",
       "      <td>2.566279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12604</td>\n",
       "      <td>You know? Our spirit is based on that revoluti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>-147.503376</td>\n",
       "      <td>-147.094281</td>\n",
       "      <td>-0.409095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17193</td>\n",
       "      <td>This U.S soap opera, 'Knots Landing' has all t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-253.363938</td>\n",
       "      <td>-254.473988</td>\n",
       "      <td>1.110050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24056</td>\n",
       "      <td>Rich ditzy Joan Winfield (a woefully miscast B...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-433.638252</td>\n",
       "      <td>-435.754354</td>\n",
       "      <td>2.116102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10641</td>\n",
       "      <td>So i consider myself pretty big into the anime...</td>\n",
       "      <td>positive</td>\n",
       "      <td>-211.822531</td>\n",
       "      <td>-211.693207</td>\n",
       "      <td>-0.129323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3185</th>\n",
       "      <td>2636</td>\n",
       "      <td>Sure, it was cheesy and nonsensical and at tim...</td>\n",
       "      <td>positive</td>\n",
       "      <td>-203.812479</td>\n",
       "      <td>-203.574805</td>\n",
       "      <td>-0.237674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3186</th>\n",
       "      <td>10665</td>\n",
       "      <td>Dull haunted house thriller finds an American ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-491.591483</td>\n",
       "      <td>-492.848655</td>\n",
       "      <td>1.257173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3187</th>\n",
       "      <td>16834</td>\n",
       "      <td>The first installment of this notorious horror...</td>\n",
       "      <td>positive</td>\n",
       "      <td>-225.706614</td>\n",
       "      <td>-224.767469</td>\n",
       "      <td>-0.939145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3188</th>\n",
       "      <td>11465</td>\n",
       "      <td>One of the lamer wedding movies you'll see. Sm...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-175.486197</td>\n",
       "      <td>-175.879419</td>\n",
       "      <td>0.393222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3189</th>\n",
       "      <td>3420</td>\n",
       "      <td>Perhaps not my genre but plot was horrible as ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-99.422032</td>\n",
       "      <td>-99.746566</td>\n",
       "      <td>0.324534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3190 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                             review sentiment  \\\n",
       "0     21492  I saw this recent Woody Allen film because I'm...  negative   \n",
       "1     12604  You know? Our spirit is based on that revoluti...  positive   \n",
       "2     17193  This U.S soap opera, 'Knots Landing' has all t...  negative   \n",
       "3     24056  Rich ditzy Joan Winfield (a woefully miscast B...  negative   \n",
       "4     10641  So i consider myself pretty big into the anime...  positive   \n",
       "...     ...                                                ...       ...   \n",
       "3185   2636  Sure, it was cheesy and nonsensical and at tim...  positive   \n",
       "3186  10665  Dull haunted house thriller finds an American ...  negative   \n",
       "3187  16834  The first installment of this notorious horror...  positive   \n",
       "3188  11465  One of the lamer wedding movies you'll see. Sm...  negative   \n",
       "3189   3420  Perhaps not my genre but plot was horrible as ...  negative   \n",
       "\n",
       "      log(P(pos|text))  log(P(neg|text))  log_Probility_error  \n",
       "0          -382.490069       -385.056348             2.566279  \n",
       "1          -147.503376       -147.094281            -0.409095  \n",
       "2          -253.363938       -254.473988             1.110050  \n",
       "3          -433.638252       -435.754354             2.116102  \n",
       "4          -211.822531       -211.693207            -0.129323  \n",
       "...                ...               ...                  ...  \n",
       "3185       -203.812479       -203.574805            -0.237674  \n",
       "3186       -491.591483       -492.848655             1.257173  \n",
       "3187       -225.706614       -224.767469            -0.939145  \n",
       "3188       -175.486197       -175.879419             0.393222  \n",
       "3189        -99.422032        -99.746566             0.324534  \n",
       "\n",
       "[3190 rows x 6 columns]"
      ]
     },
     "execution_count": 726,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analytsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3190.000000\n",
       "mean        2.164316\n",
       "std         2.286472\n",
       "min         0.001339\n",
       "25%         0.642621\n",
       "50%         1.425189\n",
       "75%         2.914307\n",
       "max        21.240403\n",
       "Name: log_Probility_error, dtype: float64"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(analytsis['log_Probility_error']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3190 false predictions, the average error of the data is about 2.164316, the standard deviation of the error is about 2.286472 and the maximum error is 21.240403. In these cases, the predicted probabilities are very far apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify some text samples on your own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part I found some audience reviews for *AVENGERS: ENDGAME* from *Rotten Tomatoes*. I use the functions classify and predict to classify these movie reviews.\n",
    "\n",
    "Audience1(4/5 stars) -- *Kevin M. W* \n",
    "\n",
    "Audience2(1/4 stars) -- *Ernesto Diezmartinez*\n",
    "\n",
    "Audience3(C+) -- *Frank Swietek*\n",
    "\n",
    "Audience4(4.5/5 stars) -- *Michael M*\n",
    "\n",
    "Audience5(4/5 stars) -- *Ranjay R* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative', 'negative', 'negative', 'positive', 'positive']"
      ]
     },
     "execution_count": 745,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Audience1 = \"The conclusion of this epic storyline is meant to be, err, epic, and in many ways it succeeds. Marvel throws a proverbial feast for it's legions of fandom for sticking through 10 years and many divergent stories, and many different heroes. Of course, they cannot do justice to everyone and so many get simply a nod or a wave, but the overall effect is still worthy - for first viewing and how ever many viewings that will come after. That the story's problem is tackled with a cliche cinematic solve...well, for entertainment's sake, and seen through the verve given execution, it's a forgivable decision.\"\n",
    "Audience2 = \"Frigga has one of the funniest lines in the whole movie: 'Son,eat more vegetables.' Indeed, it is not terrible eat junk food from time to time, but we must eat healthier. The same goes for cinema. Endgame is junk food. The healthier cinema is elsewhere.\"\n",
    "Audience3 = \"Predictable and...overstuffed with snarky lines and bathetic moments, though as sumptuously made as anything to have rolled off the Marvel assembly line [but] since it gives series fans a heaping helping of what they obviously savor, they'll eat it up.\"\n",
    "Audience4 = \"As a conclusion to over a decades worth of storytelling, it's got a lot to live up to, and it's about as good as you could really hope for it to be. Overall, I really do like Infinity War better. At the end of the day, Infinity War was just a wildly new experience, it was such a gamechanger and crowd-pleaser. Not only that, Infinity War I think had a better final battle. This final battle, while boasting the biggest ensemble of Marvel heroes we've ever seen on screen, is weirdly dark and muddied in CGI scenery compared to the battle at Wakanda from Infinity War. Also I hate Professor Hulk, he's just not that fun. Okay so those are my criticisms, what did I like about the movie? I guess everything else? The time travel plot let's us revisit and honor some old moments in Marvel with a new lens in sort of a Back to the Future Part 2 way that's a lot of fun, and it brings back some characters I didn't think we'd ever see again. The movie has a big emotional core, and a climactic end that is godamn powerful. This movie juggles a lot. Seriously this is like watching someone juggle 10 flaming chainsaws, it's a godamn miracle it works at all. That it works this well is even more of a miracle. It's an imperfect movie, but perfectly satisfying conclusion to this arc of the Marvel saga.\"\n",
    "Audience5 = \"There isn't too much in the way of suspense or surprises when it comes to the story, but there are some shocking moments and funny lines in this epic finale. Again, like many of the best Marvel films, the holes and flaws are covered up with humor and fan service, making everything okay. That being said, I did prefer Infinity War to this film, which really misses the leads of the other Marvel franchises that were 'snapped' out. Overall, however, there are only a few ways you can wrap up the main story of the MCU, and this was a solid direction.\"\n",
    "\n",
    "predict_list = []\n",
    "\n",
    "A1 = classify(Audience1).sum()\n",
    "P1 = predict(A1)\n",
    "A2 = classify(Audience2).sum()\n",
    "P2 = predict(A2)\n",
    "A3 = classify(Audience3).sum()\n",
    "P3 = predict(A3)\n",
    "A4 = classify(Audience4).sum()\n",
    "P4 = predict(A4)\n",
    "A5 = classify(Audience5).sum()\n",
    "P5 = predict(A5)\n",
    "predict_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most predictions are accurate, the first prediction is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this report is to examine the ability of a Naive Bayes classifier to predict whether a movie review is positive or negative based on the review text. Also, check how Naive Bayes predicts which newsgroup a post is submitted to based on the text of the post. My main focus is to make predictions by naive Bayes classifier by the frequency of words and then check the accuracy of the predictions. According to my experimental results: using the Naive Bayes classifier to predict whether a movie review is positive or negative is 54.05% accurate, and using the text of the post to predict which newsgroup a post is submitted to is 23.17% accurate. The five words that appeared most frequently in positive reviews also appeared a lot in negative reviews. And in the case of a wrong prediction, we can see that the predicted probabilities are far apart, with a mean (log(P)) error of 2.164316."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
